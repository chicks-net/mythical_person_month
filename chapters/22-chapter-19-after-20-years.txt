19
The Mythical Man-Month
after 20 Years

19
The Mythical Man-Month
after 20 Years
I know no way of judging the future but by the past.
PATRICK HENRY
You can never plan the future by the past.
EDMUND BURKE
Shooting the Rapids
The Bettman Archive
253
254 The Mythical Man-Month after 20 Years
Why Is There a Twentieth Anniversary Edition?
The plane droned through the night toward LaGuardia. Clouds
and darkness veiled all interesting sights. The document I was
studying was pedestrian. I was not, however, bored. The stranger
sitting next to me was reading The Mythical Man-Month, and I
was waiting to see if by word or sign he would react. Finally as
we taxied toward the gate, I could wait no longer:
"How is that book? Do you recommend it?"
"Hmph! Nothing in it I didn't know already."
I decided not to introduce myself.
Why has The Mythical Man-Month persisted? Why is it still
seen to be relevant to software practice today? Why does it have
a readership outside the software engineering community, gen-
erating reviews, citations, and correspondence from lawyers,
doctors, psychologists, sociologists, as well as from software
people? How can a book written 20 years ago about a software-
building experience 30 years ago still be relevant, much less use-
ful?
One explanation sometimes heard is that the software de-
velopment discipline has not advanced normally or properly.
This view is often supported by contrasting computer software
development productivity with computer hardware manufac-
turing productivity, which has multiplied at least a thousand-
fold over the two decades. As Chapter 16 explains, the anomaly
is not that software has been so slow in its progress but rather
that computer technology has exploded in a fashion unmatched
in human history. By and large this comes from the gradual
transition of computer manufacturing from an assembly indus-
try to a process industry, from labor-intensive to capital-inten-
sive manufacturing. Hardware and software development, in
contrast to manufacturing, remain inherently labor-intensive.
A second explanation often advanced is that The Mythical
Man-Month is only incidentally about software but primarily
about how people in teams make things. There is surely some
truth in this; the preface to the 1975 edition says that managing
The Central Argument: Conceptual Integrity 255
a software project is more like other management than most
programmers initially believe. I still believe that to be true. Hu-
man history is a drama in which the stories stay the same, the
scripts of those stories change slowly with evolving cultures,
and the stage settings change all the time. So it is that we see
our twentieth-century selves mirrored in Shakespeare, Homer,
and the Bible. So to the extent The MM-M is about people and
teams, obsolescence should be slow.
Whatever the reason, readers continue to buy the book, and
they continue to send me much-appreciated comments. Nowa-
days I am often asked, "What do you now think was wrong
when written? What is now obsolete? What is really new in the
software engineering world?" These quite distinct questions are
all fair, and I shall address them as best I can. Not in that order,
however, but in clusters of topics. First, let us consider what was
right when written, and still is.
The Central Argument: Conceptual Integrity and
the Architect
Conceptual integrity. A clean, elegant programming product
must present to each of its users a coherent mental model of the
application, of strategies for doing the application, and of the
user-interface tactics to be used in specifying actions and param-
eters. The conceptual integrity of the product, as perceived by
the user, is the most important factor in ease of use. (There are
other factors, of course. The Macintosh's uniformity of user in-
terface across all applications is an important example. More-
over, it is possible to construct coherent interfaces that are
nevertheless quite awkward. Consider MS-DOS.)
There are many examples of elegant software products de-
signed by a single mind, or by a pair. Most purely intellectual
works such as books or musical compositions are so produced.
Product-development processes in many industries cannot,
however, afford this straightforward approach to conceptual in-
tegrity. Competitive pressures force urgency; in many modern
256 The Mythical Man-Month after 20 Years
technologies the end product is quite complex, and the design
inherently requires many man-months of effort. Software prod-
ucts are both complex and fiercely competitive in schedule.
Any product that is sufficiently big or urgent to require the
effort of many minds thus encounters a peculiar difficulty: the
result must be conceptually coherent to the single mind of the
user and at the same time designed by many minds. How does
one organize design efforts so as to achieve such conceptual in-
tegrity? This is the central question addressed by The MM-M.
One of its theses is that managing large programming projects
is qualitatively different from managing small ones, just because
of the number of minds involved. Deliberate, and even heroic,
management actions are necessary to achieve coherence.
The architect. I argue in Chapters 4 through 7 that the most
important action is the commissioning of some one mind to be
the product's architect, who is responsible for the conceptual in-
tegrity of all aspects of the product perceivable by the user. The
architect forms and owns the public mental model of the prod-
uct that will be used to explain its use to the user. This includes
the detailed specification of all of its function and the means for
invoking and controlling it. The architect is also the user's agent,
knowledgeably representing the user's interest in the inevitable
tradeoffs among function, performance, size, cost, and sched-
ule. This role is a full-time job, and only on the smallest teams
can it be combined with that of the team manager. The architect
is like the director and the manager like the producer of a mo-
tion picture.
Separation of architecture from implementation and realiza-
tion. To make the architect's crucial task even conceivable, it is
necessary to separate the architecture, the definition of the
product as perceivable by the user, from its implementation. Ar-
chitecture versus implementation defines a clean boundary be-
tween parts of the design task, and there is plenty of work on
each side of it.
The Second-System Effect 257
Recursion of architects. For quite large products, one mind
cannot do all of the architecture, even after all implementation
concerns have been split off. So it is necessary for the system
master architect to partition the system into subsystems. The
subsystem boundaries must be at those places where interfaces
between the subsystems are minimal and easiest to define rig-
orously. Then each piece will have its own architect, who must
report to the system master architect with respect to the archi-
tecture. Clearly this process can proceed recursively as required.
Today I am more convinced than ever. Conceptual integrity is
central to product quality. Having a system architect is the most
important single step toward conceptual integrity. These prin-
ciples are by no means limited to software systems, but to the
design of any complex construct, whether a computer, an air-
plane, a Strategic Defense Initiative, a Global Positioning Sys-
tem. After teaching a software engineering laboratory more than
20 times, I came to insist that student teams as small as four peo-
ple choose a manager and a separate architect. Defining distinct
roles in such small teams may be a little extreme, but I have ob-
served it to work well and to contribute to design success even
for small teams.
The Second-System Effect: Featuritis and
Frequency-Guessing
Designing for large user sets. One of the consequences of the
personal computer revolution is that increasingly, at least in the
business data processing community, off-the-shelf packages
are replacing custom applications. Moreover, standard software
packages sell hundreds of thousands of copies, or even millions.
System architects for machine-vendor-supplied software have
always had to design for a large, amorphous user set rather than
for a single, definable application in one company. Many, many
system architects now face this task.
Paradoxically, it is much more difficult to design a general-
258 The Mythical Man-Month after 20 Years
purpose tool than it is to design a special-purpose tool, precisely
because one has to assign weights to the differing needs of the
diverse users.
Featuritis. The besetting temptation for the architect of a gen-
eral purpose tool such as a spreadsheet or a word processor is
to overload the product with features of marginal utility, at the
expense of performance and even of ease of use. The appeal of
proposed features is evident at the outset; the performance pen-
alty is evident only as system testing proceeds. The loss of ease
of use sneaks up insidiously, as features are added in little in-
crements, and the manuals wax fatter and fatter.1
For mass-market products that survive and evolve through
many generations, the temptation is especially strong. Millions
of customers request hundreds of features; any request is itself
evidence that "the market demands it." Frequently, the original
system architect has gone on to greater glories, and the archi-
tecture is in the hands of people with less experience at repre-
senting the user's overall interest in balance. A recent review of
Microsoft Word 6.0 says "Word 6.0 packs in features; update
slowed by baggage. . . . Word 6.0 is also big and slow." It notes
with dismay that Word 6.0 requires 4 MB of RAM, and goes on
to say that the rich added function means that "even a Macin-
tosh Ilfx [is] just barely up to the Word 6 task".2
Defining the user set. The larger and more amorphous the
user set, the more necessary it is to define it explicitly if one is
to achieve conceptual integrity. Each member of the design team
will surely have an implicit mental image of the users, and each
designer's image will be different. Since an architect's image of
the user consciously or subconsciously affects every architec-
tural decision, it is essential for a design team to arrive at a sin-
gle shared image. And that requires writing down the attributes
of the expected user set, including:
* Who they are
* What they need
The Second-System Effect 259
* What they think they need
* What they want
Frequencies. For any software product, any of the attributes of
the user set is in fact a distribution, with many possible values,
each with its own frequency. How is the architect to arrive at
these frequencies? Surveying this ill-defined population is a du-
bious and costly proposition.3 Over the years I have become
convinced that an architect should guess, or, if you prefer, pos-
tulate, a complete set of attributes and values with their fre-
quencies, in order to develop a complete, explicit, and shared
description of the user set.
Many benefits flow from this unlikely procedure. First, the
process of carefully guessing the frequencies will cause the ar-
chitect to think very carefully about the expected user set. Sec-
ond, writing the frequencies down will subject them to debate,
which will illuminate all the participants and bring to the sur-
face the differences in the user images that the several designers
carry. Third, enumerating the frequencies explicitly helps every-
one recognize which decisions depend upon which user set
properties. Even this sort of informal sensitivity analysis is valu-
able. When it develops that very important decisions are hing-
ing on some particular guess, then it is worth the cost to
establish better estimates for that value. (The gIBIS system de-
veloped by Jeff Conklin provides a tool for formally and accu-
rately tracking design decisions and documenting the reasons
for each.4 I have not had opportunity to use it, but I think it
would be very helpful.)
To summarize: write down explicit guesses for the attributes
of the user set. It is far better to be explicit and wrong man to be
vague.
What about the "Second-System Effect"? A perceptive stu-
dent remarked that The Mythical Man-Month recommended a
recipe for disaster: Plan to deliver the second version of any new
system (Chapter 11), which Chapter 5 characterizes as the most
260 The Mythical Man-Month after 20 Years
dangerous system one ever designs. I had to grant him a
"gotcha."
The contradiction is more linguistic than real. The "second"
system described in Chapter 5 is the second system fielded, the
follow-on system that invites added function and frills. The
"second" system in Chapter 11 is the second try at building
what should be the first system to be fielded. It is built under all
the schedule, talent, and ignorance constraints that characterize
new projects—the constraints that exert a slimness discipline.
The Triumph of the WIMP Interface
One of the most impressive developments in software during
the past two decades has been the triumph of the Windows,
Icons, Menus, Pointing interface—or WIMP for short. It is today
so familiar as to need no description. This concept was first
publicly displayed by Doug Englebart and his team from the
Stanford Research Institute at the Western Joint Computer
Conference of 1968.5 From there the ideas went to Xerox Palo
Alto Research Center, where they emerged in the Alto personal
workstation developed by Bob Taylor and team. They were
picked up by Steve Jobs for the Apple Lisa, a computer too slow
to carry its exciting ease-of-use concepts. These concepts Jobs
then embodied in the commercially successful Apple Macintosh
in 1985. They were later adopted in Microsoft Windows for the
IBM PC and compatibles. The Mac version will be my example.6
Conceptual integrity via a metaphor. The WIMP is a superb
example of a user interface that has conceptual integrity,
achieved by the adoption of a familiar mental model, the desk-
top metaphor, and its careful consistent extension to exploit a
computer graphics implementation. For example, the costly but
proper decision to overlay windows instead of tiling them fol-
lows directly from the metaphor. The ability to size and shape
windows is a consistent extension that gives the user the new
powers enabled by the computer graphics medium. Papers on a
The Triumph of the WIMP Interface 261
desktop cannot be so readily sized and shaped. Dragging and
dropping follow directly from the metaphor; selecting icons by
pointing with a cursor is a direct analog of picking things with
the hand. Icons and nested folders are faithful analogs of desk-
top documents; so is the trash can. The concepts of cutting,
copying, and pasting faithfully mirror the things we used to do
with documents on desktops. So faithfully is the metaphor fol-
lowed and so consistent is its extension that new users are pos-
itively jarred by the notion of dragging a diskette's icon to the
trash to eject the disk. Were the interface not almost uniformly
consistent, that (pretty bad) inconsistency would not grate so
much.
Where is the WIMP interface forced to go far beyond the
desktop metaphor? Most notably in two respects: menus and
one-handedness. When working with a real desktop, one does
actions to documents, rather than telling someone or something
to do them. And when one does tell someone to do an action,
one usually generates, rather than selects, the oral or written
imperative verb commands: "Please file this." "Please find the
earlier correspondence." "Please send this to Mary to handle."
Alas, the reliable interpretation of free-form generated En-
glish commands is beyond the present state of the art, whether
commands are written or spoken. So the interface designers
were two steps removed from direct user action on documents.
They wisely picked up from the usual desktop its one example
of command selection—the printed buck slip, on which the user
selects from among a constrained menu of commands whose se-
mantics are standardized. This idea they extended to a horizon-
tal menu of vertical pull-down submenus.
Command utterances and the two-cursor problem. Com-
mands are imperative sentences; they always have a verb and
usually have a direct object.. For any action, one needs to specify
a verb and a noun. The pointing metaphor says, to specify two
things at a time, have two distinguished cursors on the screen,
each driven by a separate mouse—one in the right hand and one
262 The Mythical Man-Month after 20 Years
in the left. After all, on a physical desktop we normally work
with both hands. (But, one hand is often holding things fixed in
place, which happens by default on the computer desktop.) The
mind is certainly capable of two-handed operation; we regularly
use two hands in typing, driving, cooking. Alas, providing one
mouse was already a big step forward for personal computer
makers; no commercial system accommodates two simultane-
ous mouse-cursor actions, one driven with each hand.7
The interface designers accepted reality and designed for
one mouse, adopting the syntactic convention that one points
out (selects) the noun first. One points at the verb, a menu item.
This really gives away a lot of ease-of-use. As I watch users, or
videotapes of users, or computer tracings of cursor movements,
I am immediately struck that one cursor is having to do the work
of two: pick an object in the desktop part of the window; pick a
verb in the menu portion; find or re-find an object in the desk-
top; again pull down a menu (often the same one) and pick a
verb. Back and forth, back and forth the cursor goes, from data-
space to menu-space, each time discarding the useful informa-
tion as to where it was last time it was in this space—altogether,
an inefficient process.
A brilliant solution. Even if the electronics and software could
readily handle two simultaneously active cursors, there are
space-layout difficulties. The desktop in the WIMP metaphor
really includes a typewriter, and one must accommodate its real
keyboard in physical space on the real desktop. A keyboard plus
two mouse-pads uses a lot of the arm's-reach real estate. Well,
the problem of the keyboard can be turned into an opportu-
nity—why not enable efficient two-handed operation by using
one hand on the keyboard to specify verbs and the other hand
on a mouse to pick nouns. Now the cursor stays in the data
space, exploiting the high locality of successive noun picks. Real
efficiency, real user power.
The Triumph of the WIMP Interface 263
User power versus ease of use. That solution, however, gives
away the thing that makes menus so easy to use for novices—
menus present the alternative verbs valid at any particular state.
We can buy a package, bring it home, and start using it without
consulting the manual, merely by knowing what we bought it
for, and experimenting with the different menu verbs.
One of the hardest issues facing software architects is ex-
actly how to balance user power versus ease of use. Do they de-
sign for simple operation for the novice or the occasional user,
or for efficient power for the professional user? The ideal answer
is to provide both, in a conceptually coherent way—that is
achieved in the WIMP interface. The high-frequency menu
verbs each have single-key + command-key equivalents, mostly
chosen so that they can easily be struck as a single chord with
the left hand. On the Mac, for example, the command key (||)
is just below the Z and X keys; therefore the highest-frequency
operations are encoded as
Incremental transition from novice to power user. This dual
system for specifying command verbs not only meets the low-
learning-effort needs of the novice and the efficiency needs of
the power user, it provides for each user to make a smooth tran-
sition between modes. The letter encodings, called short cuts, are
shown on the menus beside the verbs, so that a user in doubt
can pull down the menu to check the letter equivalent, instead
of just picking the menu item. Each novice learns first the short
cuts for his own high-frequency operations. Any short cut he is
doubtful about he can try, since &z will undo any single mis-
take. Alternatively, he can check the menu to see what com-
mands are valid. Novices will pull lots of menus; power users
very few; and in-between users will only occasionally need to
pick from a menu, since each will know the few short-cuts that
make up most of his own operations. Most of us software de-
signers are too familiar with this interface to appreciate fully its
elegance and power.
264 The Mythical Man-Month after 20 Years
The success of direct incorporation as a device for enforcing ar-
chitecture. The Mac interface is remarkable in yet another way.
Without coercion, its designers have made it a standard inter-
face across applications, including the vast majority that are
written by third parties. So the user gains conceptual coherence
at the interface level not only within the software furnished with
the machine but across all applications.
This feat the Mac designers accomplished by building the
interface into the read-only memory, so that it is easier and
faster for developers to use it than to build their own idiosyn-
cratic interfaces. These natural incentives for uniformity pre-
vailed widely enough to establish a de facto standard. The
natural incentives were helped by a total management commit-
ment and a lot of persuasion by Apple. The independent re-
viewers in the product magazines, recognizing the immense
value of cross-application conceptual integrity, have also sup-
plemented the natural incentives by mercilessly criticizing prod-
ucts that do not conform.
This is a superb example of the technique, recommended in
Chapter 6, of achieving uniformity by encouraging others to di-
rectly incorporate one's code into their products, rather than at-
tempting to have them build their own software to one's
specifications.
The fate of WIMP: Obsolescence. Despite its excellencies, I ex-
pect the WIMP interface to be a historical relic in a generation.
Pointing will still be the way to express nouns as we command
our machines; speech is surely the right way to express the
verbs. Tools such as Voice Navigator for the Mac and Dragon for
the PC already provide this capability.
Don't Build One to Throw Away—The Waterfall Model
Is Wrong!
The unforgettable picture of Galloping Gertie, the Tacoma Nar-
rows Bridge, opens Chapter 11, which radically recommends:
Don't Build One to Throw Away 265
"Plan to throw one away; you will, anyhow." This I now per-
ceive to be wrong, not because it is too radical, but because it is
too simplistic.
The biggest mistake in the "Build one to throw away" con-
cept is that it implicitly assumes the classical sequential or wa-
terfall model of software construction. The model derives from
a Gantt chart layout of a staged process, and it is often drawn
as in Figure 19.1. Win ton Royce improved the sequential model
in a classic 1970 paper by providing for
* Some feedback from a stage to its predecessors
* Limiting the feedback to the immediately preceding stage
only, so as to contain the cost and schedule delay it occa-
sions.
He preceded The MM-M in advising builders to "build it twice."8
Chapter 11 is not the only one tainted by the sequential waterfall
model; it runs through the book, beginning with the scheduling
rule in Chapter 2. That rule-of-thumb allocates 1/3 of the sched-
ule to planning, 1/6 to coding, 1/4 to component test, and 1/4 to
system test.
Fig. 19.1 Waterfall model of software construction
266 The Mythical Man-Month after 20 Years
The basic fallacy of the waterfall model is that it assumes a
project goes through the process once, that the architecture is ex-
cellent and easy to use, the implementation design is sound,
and the realization is fixable as testing proceeds. Another way
of saying it is that the waterfall model assumes the mistakes will
all be in the realization, and thus that their repair can be
smoothly interspersed with component and system testing.
"Plan to throw one away" does indeed attack this fallacy
head on. It is not the diagnosis that is wrong; it is the remedy.
Now I did suggest that one might discard and redesign the first
system piece by piece, rather than in one lump. This is fine so
far as it goes, but it fails to get at the root of the problem. The
waterfall model puts system test, and therefore by implication
user testing, at the end of the construction process. Thus one can
find impossible awkwardnesses for users, or unacceptable per-
formance, or dangerous susceptibility to user error or malice,
only after investing in full construction. To be sure, the Alpha
test scrutiny of the specifications aims to find such flaws early,
but there is no substitute for hands-on users.
The second fallacy of the waterfall model is that it assumes
one builds a whole system at once, combining the pieces for an
end-to-end system test after all of the implementation design,
most of the coding, and much of the component testing has
been done.
The waterfall model, which was the way most people
thought about software projects in 1975, unfortunately got en-
shrined into DOD-STD-2167, the Department of Defense speci-
fication for all military software. This ensured its survival well
past the time when most thoughtful practitioners had recog-
nized its inadequacy and abandoned it. Fortunately, the DoD
has since begun to see the light.9
There has to be upstream movement. Like the energetic
salmon in the chapter-opening picture, experience and ideas
from each downstream part of the construction process must
leap upstream, sometimes more than one stage, and affect the
An Incremental-Build Model Is Better 267
upstream activity.
Designing the implementation will show that some archi-
tectural features cripple performance; so the architecture has to
be reworked. Coding the realization will show some functions
to balloon space requirements; so there may have to be changes
to architecture and implementation.
One may well, therefore, iterate through two or more ar-
chitecture-implementation design cycles before realizing any-
thing as code.
An Incremental-Build Model Is Better—
Progressive Refinement
Building an end-to-end skeleton system
Harlan Mills, working in a real-time system environment, early
advocated that we should build the basic polling loop of a real-
time system, with subroutine calls (stubs) for all the functions
(Fig. 19.2), but only null subroutines. Compile it; test it. It goes
round and round, doing literally nothing, but doing it cor-
rectly.10
Subroutines
Fig. 19.2
268 The Mythical Man-Month after 20 Years
Next, we flesh out a (perhaps primitive) input module and
an output module. Voil3! A running system that does some-
thing, however dull. Now, function by function, we incremen-
tally build and add modules. At every stage we have a running
system. If we are diligent, we have at every stage a debugged,
tested system. (As the system grows, so does the burden of
regression-testing each new module against all the previous test
cases.)
After every function works at a primitive level, we refine or
rewrite first one module and then another, incrementally grow-
ing the system. Sometimes, to be sure, we have to change the
original driving loop, and or even its module interfaces.
Since we have a working system at all times
• we can begin user testing very early, and
• we can adopt a build-to-budget strategy that protects abso-
lutely against schedule or budget overruns (at the cost of
possible functional shortfall).
For some 22 years, I taught the software engineering laboratory
at the University of North Carolina, sometimes jointly with Da-
vid Parnas. In this course, teams of usually four students built
in one semester some real software application system. About
halfway through those years, I switched to teaching incremental
development. I was stunned by the electrifying effect on team
morale of that first picture on the screen, that first running sys-
tem.
Parnas Families
David Parnas has been a major thought leader in software en-
gineering during this whole 20-year period. Everyone is familiar
with his information-hiding concept. Rather less familiar, but
very important, is Parnas's concept of designing a software
product as a family of related products.11 He urges the designer
to anticipate both lateral extensions and succeeding versions of
Parnas Families 269
Fig. 19.3
a product, and to define their function or platform differences
so as to construct a family tree of related products (Fig 19.3).
The trick in the design of such a tree is to put near its root
those design decisions that are less likely to change.
Such a design strategy maximizes the re-use of modules.
More important, the same strategy can be broadened to include
not only deliverable products but also the successive interme-
diate versions created in an incremental-build strategy. The
product then grows through its intermediate stages with mini-
mum backtracking.
270 The Mythical Man-Month after 20 Years
Microsoft's "Build Every Night" Approach
James McCarthy described to me a product process used by his
team and others at Microsoft. It is incremental growth carried to
a logical conclusion. He says,
After we first ship, we will be shipping later versions that add more
function to an existing, running product. Why should the initial
building process be different? Beginning at the time of our first
milestone [where the march to first ship has three intermediate
milestones] we rebuild the developing system every night [and run
the test cases]. The build cycle becomes the heartbeat of the project.
Every day one or more of the programmer-tester teams check in
modules with new functions. After every build, we have a running
system. If the build breaks, we stop the whole process until the trou-
ble is found and fixed. At all times everybody on the team knows
the status.
It is really hard. You have to devote lots of resources, but it is
a disciplined process, a tracked and known process. It gives the
team credibility to itself. Your credibility determines your morale,
your emotional state.
Software builders in other organizations are surprised, even
shocked, by this process. One says, "I've made it a practice to
build every week, but I think it would be too much work to
build every night." And that may be true. Bell Northern Re-
search, for example, rebuilds its 12-million-line system every
week.
Incremental-Build and Rapid Prototyping
Since an incremental development process enables early testing
with real users, what is the difference between that and rapid
prototyping? It seems to me that the two are related but sepa-
rate. One can have either without the other.
Harel usefully defines a prototype as
[A version of a program that] reflects only the design decisions
made in the process of preparing the conceptual model, and not de-
cisions driven by implementation concerns.12
Parnas Was Right, and I Was Wrong 271
It is possible to build a prototype that is not at all part of a prod-
uct growing toward shipment. For example, one may build an
interface prototype that has no real program function behind it,
merely the finite-state machine that makes it appear to go
through its paces. One can even prototype and test interfaces
by the Wizard-of-Oz technique, with a concealed human simu-
lating the system's responses. Such prototyping can be very use-
ful for getting early user feedback, but it is quite apart from
testing a product growing toward shipment.
Similarly, implementers may well undertake to build a ver-
tical slice of a product, in which a very limited function set is
constructed in full, so as to let early sunlight into places where
performance snakes may lurk. What is the difference between
the first-milestone-build of the Microsoft process and a rapid
prototype? Function. The first-milestone product may not have
enough function to be of interest to anyone; the shippable prod-
uct is defined as such by its completeness in furnishing a useful
set of functions, and by its quality, the belief that it works ro-
bustly.
Parnas Was Right, and I Was Wrong about
Information Hiding
In Chapter 7 I contrast two approaches to the question of how
much each team member should be allowed or encouraged to
know about each other's designs and code. In the Operating
System/360 project, we decided that all programmers should see
all the material—i.e., each programmer having a copy of the
project workbook, which came to number over 10,000 pages.
Harlan Mills has argued persuasively that "programming
should be a public process," that exposing all the work to every-
body's gaze helps quality control, both by peer pressure to do
things well and by peers actually spotting flaws and bugs.
This view contrasts sharply with David Parnas's teaching
that modules of code should be encapsulated with well-defined
interfaces, and that the interior of such a module should be the
272 The Mythical Man-Month after 20 Years
private property of its programmer, not discernible from out-
side. Programmers are most effective if shielded from, not ex-
posed to, the innards of modules not their own.13
I dismissed Parnas's concept as a "recipe for disaster" in
Chapter 7. Parnas was right, and I was wrong. I am now con-
vinced that information hiding, today often embodied in object-
oriented programming, is the only way of raising the level of
software design.
One can indeed get disasters with either technique. Mills'
technique ensures that programmers can know the detailed se-
mantics of the interfaces they work to by knowing what is on
the other side. Hiding those semantics leads to system bugs. On
the other hand, Parnas's technique is robust under change and
is more appropriate in a design-for-change philosophy.
Chapter 16 argues the following:
• Most past progress in software productivity has come from
eliminating noninherent difficulties such as awkward ma-
chine languages and slow batch turnaround.
• There are not a lot more of these easy pickings.
• Radical progress is going to have to come from attacking the
essential difficulties of fashioning complex conceptual con-
structs.
The most obvious way to do this recognizes that programs
are made up of conceptual chunks much larger than the individ-
ual high-level language statement—subroutines, or modules, or
classes. If we can limit design and building so that we only do
the putting together and parameterization of such chunks from
prebuilt collections, we have radically raised the conceptual
level, and eliminated the vast amounts of work and the copious
opportunities for error that dwell at the individual statement
level.
Parnas's information-hiding definition of modules is the
first published step in that crucially important research pro-
gram, and it is an intellectual ancestor of object-oriented pro-
How Mythical Is the Man-Month? 273
gramming. He defined a module as a software entity with its
own data model and its own set of operations. Its data can only
be accessed via one of its proper operations. The second step
was a contribution of several thinkers: the upgrading of the Par-
nas module into an abstract data type, from which many objects
could be derived. The abstract data type provides a uniform way
of thinking about and specifying module interfaces, and an ac-
cess discipline that is easy to enforce.
The third step, object-oriented programming, introduces
the powerful concept of inheritance, whereby classes (data types)
take as defaults specified attributes from their ancestors in the
class hierarchy.14 Most of what we hope to gain from object-
oriented programming derives in fact from the first step, mod-
ule encapsulation, plus the idea of prebuilt libraries of modules
or classes that are designed and tested for reuse. Many people have
chosen to ignore the fact that such modules are not just pro-
grams, but instead are program products in the sense discussed
in Chapter 1. Some people are vainly hoping for significant
module reuse without paying the initial cost of building prod-
uct-quality modules—generalized, robust, tested, and docu-
mented. Object-oriented programming and reuse are discussed
in Chapters 16 and 17.
How Mythical Is the Man-Month? Boehm's Model
and Data
Over the years, there have been many quantitative studies of
software productivity and the factors affecting it, especially the
trade-offs between project staffing and schedule.
The most substantial study is one done by Barry Boehm of
some 63 software projects, mostly aerospace, with about 25 at
TRW. His Software Engineering Economics contains not only the
results but a useful set of cost models of progressive compre-
hensiveness. Whereas the coefficients in the models are surely
different for ordinary commercial software and for aerospace
software built to government standards, nevertheless his mod-
274 The Mythical Man-Month after 20 Years
els are backed by an immense amount of data. I think the book
will be a useful classic a generation from now.
His results solidly confirm The MM-M's assertion that the
trade-off between men and months is far from linear, that the
man-month is indeed mythical as a measure of productivity. In
particular, he finds:15
• There is a cost-optimum schedule time to first shipment,
1 = 2.5 (MM) 1/3. That is, the optimum time in months goes
as the cube root of the expected effort in man-months, a fig-
ure derived from the size estimate and other factors in his
model. An optimum staffing curve is a corollary.
e The cost curve rises slowly as the planned schedule gets
longer than the optimum. People with more time take more
time.
9 The cost curve rises sharply as the planned schedule gets
shorter than the optimum.
• Hardly any projects succeed in less than 3/4 of the calculated opti-
mum schedule, regardless of the number of people applied! This
quotable result gives the software manager solid ammuni-
tion when higher management is demanding impossible
schedule commitments.
How true is Brooks's Law? There have even been careful stud-
ies evaluating the truth of Brooks's (intentionally simplistic)
Law, that adding manpower to a late software project makes it
later. The best treatment is that of Abdel-Hamid and Madnick,
in their ambitious and valuable 1991 book, Software Project Dy-
namics: An Integrated Approach.16 The book develops a quantita-
tive model of project dynamics. Their chapter on Brooks's Law
provides more detailed insight into what happens under vari-
ous assumptions as to what manpower is added, and when. To
investigate this, the authors extend their own careful model of
a middle-sized applications project by assuming that new peo-
ple have a learning curve and accounting for the extra commu-
How Mythical Is the Man-Month? 275
nication and training work. They conclude that "Adding more
people to a late project always makes it more costly, but it does
not always cause it to be completed later [italics theirs]." In par-
ticular, adding extra manpower early in the schedule is a much
safer maneuver than adding it later, since the new people al-
ways have an immediate negative effect, which takes weeks to
compensate.
Stutzke develops a simpler model in order to perform a sim-
ilar investigation, with a similar result.17 He develops a detailed
analysis of the process and costs of assimilating the new work-
ers, including explicitly the diversion of their mentors from the
project task itself. He tests his model against an actual project in
which manpower was successfully doubled and the original
schedule achieved, after a mid-project slip. He treats alterna-
tives to adding more programmers, especially overtime. Most
valuable are his many items of practical advice as to how new
workers should be added, trained, supported with tools, etc.,
so as to minimize the disruptive effects of adding them. Espe-
cially noteworthy is his comment that new people added late in
a development project must be team players willing to pitch in
and work within the process, and not attempt to alter or im-
prove the process itself!
Stutzke believes that the added burden of communication in
a larger project is a second-order effect and does not model it. It
is not clear whether and how Abdel-Hamid and Madnick take it
into account. Neither model takes into account the fact that the
work must be repartitioned, a process I have often found to be
non trivial.
The "outrageously simplified" statement of Brooks's Law is
made more useful by these careful treatments of the proper
qualifications. On balance, I stand by the bald statement as the
best zeroth-order approximation to the truth, a rule of thumb to
warn managers against blindly making the instinctive fix to a
late project.
276 The Mythical Man-Month after 20 Years
People Are Everything (Well, Almost Everything)
Some readers have found it curious that The MM-M devotes
most of the essays to the managerial aspects of software engi-
neering, rather than the many technical issues. This bias was
due in part to the nature of my role on the IBM Operating Sys-
tem/360 (now MVS/370). More fundamentally, it sprang from a
conviction that the quality of the people on a project, and their
organization and management, are much more important fac-
tors in success than are the tools they use or the technical ap-
proaches they take.
Subsequent researches have supported that conviction.
Boehm's COCOMO model finds that the quality of the team is
by far the largest factor in its success, indeed four times more
potent than the next largest factor. Most academic research on
software engineering has concentrated on tools. I admire and
covet sharp tools. Nevertheless, it is encouraging to see ongoing
research efforts on the care, growing, and feeding of people,
and on the dynamics of software management.
Peopleware. A major contribution during recent years has
been DeMarco and Lister's 1987 book, Peopleware: Productive
Projects and Teams. Its underlying thesis is that "The major prob-
lems of our work are not so much technological as sociological in
nature." It abounds with gems such as, "The manager's func-
tion is not to make people work, it is to make it possible for peo-
ple to work." It deals with such mundane topics as space,
furniture, team meals together. DeMarco and Lister provide real
data from their Coding War Games that show stunning corre-
lation between performances of programmers from the same or-
ganization, and between workplace characteristics and both
productivity and defect levels.
The top performers' space is quieter, more private, better protected
against interruption, and there is more of it. . . . Does it really
matter to you . . . whether quiet, space, and privacy help your cur-
rent people to do better work or [alternatively] help you to attract
and keep better people?19
The Power of Giving Up Power 277
I heartily commend the book to all my readers.
Moving projects. DeMarco and Lister give considerable atten-
tion to team fusion, an intangible but vital property. I think it is
management's overlooking fusion that accounts for the readi-
ness I have observed in multilocation companies to move a proj-
ect from one laboratory to another.
My experience and observation are limited to perhaps a
half-dozen moves. I have never seen a successful one. One can
move missions successfully. But in every case of attempts to
move projects, the new team in fact started over, in spite of hav-
ing good documentation, some well-advanced designs, and
some of the people from the sending team. I think it is the break-
ing of fusion of the old team that aborts the embryonic product,
and brings about restart.
The Power of Giving Up Power
If one believes, as I have argued at many places in this book,
that creativity comes from individuals and not from structures
or processes, then a central question facing the software man-
ager is how to design structure and process so as to enhance,
rather than inhibit, creativity and initiative. Fortunately, this
problem is not peculiar to software organizations, and great
thinkers have worked on it. E. F. Schumacher, in his classic,
Small is Beautiful: Economics as if People Mattered, proposes a the-
ory of organizing enterprises to maximize the creativity and
joy of the workers. For his first principle he chooses the "Prin-
ciple of Subsidiary Function" from the Encyclical Quadragesima
Anno of Pope Pius XI:
It is an injustice and at the same time a grave evil and disturbance
of right order to assign to a greater and higher association what
lesser and subordinate organizations can do. For every social activ-
ity ought of its very nature to furnish help to the members of the
body social and never destroy and absorb them. . . . Those in com-
mand should be sure that the more perfectly a graduated order is
278 The Mythical Man-Month after 20 Years
preserved among the various associations, in observing the princi-
ple of subsidiary function, the stronger will be the social authority
and effectiveness and the happier and more prosperous the condition
of the State.19
Schumacher goes on to interpret:
The Principle of Subsidiary Function teaches us that the centre will
gain in authority and effectiveness if the freedom and responsibility
of the lower formations are carefully preserved, with the result that
the organization as a whole will be "happier and more prosperous."
How can such a structure be achieved?. . . . The large orga-
nization will consist of many semi-autonomous units, which we
may call quasi-firms. Each of them will have a large amount of
freedom, to give the greatest possible chance to creativity and en-
trepreneurship. . . . Each quasi-firm must have both a profit and
loss account, and^a balance sheet.20
Among the most exciting developments in software engi-
neering are the early stages of putting such organizational ideas
into practice. First, the microcomputer revolution created a new
software industry of hundreds of start-up firms, all of them
starting small, and marked by enthusiasm, freedom, and crea-
tivity. The industry is changing now, as many small companies
are being acquired by larger ones. It remains to be seen if the
larger acquirers will understand the importance of preserving
the creativity of smallness.
More remarkably, high management in some large firms
have undertaken to delegate power down to individual software
project teams, making them approach Schumacher's quasi-firms
in structure and responsibility. They are surprised and de-
lighted at the results.
Jim McCarthy of Microsoft described to me his experience at
emancipating his teams:
Each feature team (30-40 people) owns its feature set, its schedule,
and even its process of how to define, build, ship. The team is made
What's the Biggest New Surprise? Millions of Computers 279
up for four or five specialties, including building, testing, and
writing. The team settles squabbles; the bosses don't. I can't em-
phasize enough the importance of empowerment, of the team being
accountable to itself for its success.
Earl Wheeler, retired head of IBM's software business, told
me his experience in undertaking the downward delegation of
power long centralized in IBM's division managements:
The key thrust [of recent years] was delegating power down. It was
like magic! Improved quality, productivity, morale. We have small
teams, with no central control. The teams own the process, but they
have to have one. They have many different processes. They own
the schedule, but they feel the pressure of the market. This pressure
causes them to reach for tools on their own.
Conversations with individual team members, of course,
show both an appreciation of the power and freedom that is del-
egated, and a somewhat more conservative estimate of how
much control really is relinquished. Nevertheless, the delega-
tion achieved is clearly a step in the right direction. It yields
exactly the benefits Pius XI predicted: the center gains in real
authority by delegating power, and the organization as a whole
is happier and more prosperous.
What's the Biggest New Surprise? Millions of Computers
Every software guru I have talked with admits to being caught
by surprise by the microcomputer revolution and its outgrowth,
the shrink-wrapped software industry. This is beyond doubt the
crucial change of the two decades since The MM-M, It has many
implications for software engineering.
The microcomputer revolution has changed how everybody
uses computers. Schumacher stated the challenge more than
20 years ago:
What is it that we really require from the scientists and technolo-
280 The Mythical Man-Month after 20 Years
gists? I should answer: We need methods and equipment which are
9 cheap enough so that they are accessible to virtually everyone;
9 suitable for small-scale application; and
• compatible with man's need for creativity.21
These are exactly the wonderful properties that the micro-
computer revolution has brought to the computer industry and
to its users, now the general public. The average American can
now afford not only a computer of his own, but a suite of soft-
ware that 20 years ago would have cost a king's salary. Each of
Schumacher's goals is worth contemplating; the degree to which
each has been achieved is worth savoring, especially the last. In
area after area, new means of self-expression are accessible to
ordinary people as well as to professionals.
Partly the enhancement comes in other fields as it has in
software creation—in the removal of accidental difficulties. Writ-
ten manuscripts used to be accidentally rigidified by the time
and cost of retyping to incorporate changes. On a 300-page
work, one might go through retyping every three to six months,
but in between, one just kept marking the manuscript. One
could not easily assess what the changes had done to the flow
of the logic and the rhythm of the words. Now, manuscripts
have become wondrously fluid.22
The computer has brought a similar fluidity to many other
media: artistic drawings, building plans, mechanical drawings,
musical compositions, photographs, video sequences, slide pre-
sentations, multimedia works, and even to spreadsheets. In
each case, the manual method of production required recopying
the bulky unchanged parts in order to see changes in context.
Now we enjoy for each medium the same benefits that time-
sharing brought to software creation—the ability to revise and
to assess instantly the effect without losing one's train of
thought.
Creativity is also enhanced by new and flexible auxiliary
tools. For prose production, as one example, we are now served
by spelling checkers, grammar checkers, style advisors, biblio-
What's the Biggest New Surprise? Millions of Computers 281
graphic systems, and the remarkable ability to see pages con-
currently formatted into final layout. We do not yet appreciate
what instantaneous encyclopedias or the infinite resources of
the World-Wide Web will mean for a writer's impromptu re-
search.
Most important, the new fluidity of the media makes easy
the exploration of many radically different alternatives when a
creative work is just taking form. Here is another case where an
order of magnitude in a quantitative parameter, here change-
time, makes a qualitative difference in how one goes about a
task.
Tools for drawing enable building designers to explore
many more options per hour of creative investment. The con-
nection of computers to synthesizers, with software for auto-
matically generating or playing scores, makes it much easier to
capture keyboard doodles. Digital photograph manipulation, as
with Adobe Photoshop, enables minutes-long experiments that
would take hours in a darkroom. Spreadsheets enable the easy
exploration of dozens of "what if" alternative scenarios.
Finally, wholly new creative media have been enabled by
the ubiquity of the personal computer. Hypertexts, proposed by
Vannevar Bush in 1945, are practical only with computers.23
Multimedia presentations and experiences were big deals—just
too much trouble—before the personal computer and the rich,
cheap software available for it. Virtual-environment systems,
not yet cheap or ubiquitous, will become so, and will be yet an-
other creative medium.
The microcomputer revolution has changed how everybody
builds software. The software processes of the 1970s have
themselves been altered by the microprocessor revolution and
the technology advances that enabled it. Many of the accidental
difficulties of those software building processes have been elim-
inated. Fast individual computers are now the routine tools of
the software developer, so that turnaround time is an almost ob-
solete concept. The personal computer of today is not only faster
282 The Mythical Man-Month after 20 Years
than the supercomputer of I960, it is faster than the Unix work-
station of 1985. All of this means that compilation is fast even
on the humblest machines, and large memories have eliminated
waits for disk-based linking. Large memories also make it rea-
sonable to keep symbol tables in memory with object code, so
high-level debugging without recompilation is routine.
In the last 20 years, we have come almost completely
through the use of time-sharing as the methodology for con-
structing software. In 1975, time-sharing had just replaced batch
computing as the most common technique. The network was
used to give the software builder access both to shared files and
to a shared powerful engine for compilation, linking, and test-
ing. Today, the personal workstation provides the computing
engine, and the network primarily gives shared access to the
files that are the team's developing work-product. Client-server
systems make shared access to check-in, build, and the appli-
cation of test cases a different and simpler process.
Similar advances in user interfaces have occurred. The
WIMP interface provides much more convenient editing of pro-
gram texts as well as of English-language texts. The 24-line,
72-column screen has been replaced by full-page or even two-
page screens, so programmers can see much more context for
changes they are making.
Whole New Software Industry—Shrink-Wrapped Software
Alongside the classical software industry there has exploded an-
other. Product unit sales run in hundreds of thousands, even
millions. Entire rich software packages can be had for less than
the cost of one supported programmer-day. The two industries
are different in many ways, and they coexist.
The classical software industry. In 1975, the software industry
had several identifiable and somewhat different components, all
of which still exist today:
« Computer vendors, who provide operating systems, com-
Shrink-Wrapped Software 283
pilers, and utilities for their products.
• Application users, such as the MIS shops of utilities, banks,
insurance companies, and government agencies, who build
application packages for their own use.
« Custom application builders, who contract to build proprie-
tary packages for users. Many of these contractors specialize
in defense applications, where requirements, standards,
and marketing procedures are peculiar.
• Commercial package developers, who at that time devel-
oped mostly large applications for specialized markets, such
as statistical analysis packages and CAD systems.
Tom DeMarco notes the fragmentation of the classical soft-
ware industry, especially the application-user component:
What I didn't expect: the field has partitioned into niches. How you
do something is much more a function of the niche than it is the use
of general systems analysis methods, general languages, and gen-
eral testing techniques. Ada was the last of the general-purpose lan-
guages, and it has become a niche language.
In the routine commercial application niche, fourth-generation
languages have made powerful contributions. Boehm says,
"Most successful 4GLs are the result of someone's codifying a
piece of an application domain in terms of options and param-
eters." The most pervasive of these 4GLs are application gen-
erators and combined database-communications packages with
inquiry languages.
Operating system worlds have coalesced. In 1975, operating
systems abounded: each hardware vendor had at least one pro-
prietary operating system per product line; many had two. How
different things are today! Open systems are the watchword,
and there are only five significant operating systems environ-
284 The Mythical Man-Month after 20 Years
ments into which people market applications packages (in
chronological order):
• The IBM MVS and VM environments
